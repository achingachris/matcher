{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents a generic LLM matchin algorithm which can be progressed as part of Swahilipot's matching of mentors and mentees, youth to opportunities etc. The algorithm has been developed to be as generic as possible, utilizing LLMs for identifying which fields \n",
    "to use in matching. It includes the following key components and being passed two dataframes:\n",
    "\n",
    "1. An automatic analysis to determine fields to use for semantic search, locations and other types. This is controlled via an LLM prompt\n",
    "2. A basic semantic search to get matches for each row in dataframe 1 in dataframe 2\n",
    "3. For each match a distance is calculated using Google maps API to filter out matches which are too distant\n",
    "4. For each remaining match, and LLM judge asseses the full record to filter out any which are not tru matches\n",
    "\n",
    "For step 1 an LLM is used, but once a `./data/matching_fields.json` is created it is used. This is also useful as it means you can manually adjust this file to include/exclude fields as-needed. This file is very important for the matching algorithm and can be a good place to tune to matching process.\n",
    "\n",
    "# Setup\n",
    "\n",
    "1. Install [miniconda](https://docs.conda.io/en/latest/miniconda.html) by selecting the installer that fits your OS version. Once it is installed you may have to restart your terminal (closing your terminal and opening again)\n",
    "2. In this directory, open terminal\n",
    "3. `conda env create -f environment.yml`\n",
    "4. `conda activate matching-env`\n",
    "5. Open this notebook in VS Code and use this environment\n",
    "\n",
    "6. You will need mentor and mentee data ...\n",
    "\n",
    "- (Mentees)[https://docs.google.com/spreadsheets/d/1i8ItmzyVEi5H0tII1C-zEWKjJJjHmeTkfPA9MDLPSLA/edit#gid=0] and the later (Mentees 2.0)(https://docs.google.com/spreadsheets/d/1U82YEHpcusuCC39mgGd0C_yAhYKf15W6MEab8kab3eU/edit#gid=0)\n",
    "- (Mentors)[https://docs.google.com/spreadsheets/d/1dEi1bsScI-gyFcLlGe-UajqzEzSb4q79ArWKrSHM4jw/edit#gid=0]\n",
    "\n",
    "These were provided by Chris from Swahilipot and have had PII removed. Download these into this folder and confirm there is no PII.\n",
    "\n",
    "7. You will need to copy `.env` to `.env.example` and set keys for APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import traceback\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "import uuid\n",
    "import googlemaps\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "MENTOR_DATA = './data/mentors.xlsx'\n",
    "MENTEE_DATA = './data/mentee 2.0.xlsx'\n",
    "DIST_MATRIX = './data/dist_matrix.json'\n",
    "MATCHING_FIELDS_JSON = './data/matching_fields.json'\n",
    "MATCHES_FILE = './data/matches.xlsx'\n",
    "\n",
    "# For distance calculation\n",
    "gmaps = googlemaps.Client(key=os.getenv('GOOGLE_MAPS_API_KEY'))\n",
    "\n",
    "def setup_models():\n",
    "    '''\n",
    "    Setup the models for the chat and embedding\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    embedding_model: AzureOpenAIEmbeddings\n",
    "    chat: AzureChatOpenAI\n",
    "\n",
    "    '''\n",
    "    embedding_model = AzureOpenAIEmbeddings(deployment=os.getenv(\"OPENAI_TEXT_COMPLETION_DEPLOYMENT_NAME\"), \n",
    "                                            azure_endpoint=os.getenv(\"BASE_URL\"), \n",
    "                                            chunk_size=16)\n",
    "\n",
    "    chat = AzureChatOpenAI(azure_deployment=os.getenv(\"OPENAI_DEPLOYMENT_NAME\"), \n",
    "                        azure_endpoint=os.getenv(\"BASE_URL\"), \n",
    "                        temperature=0, max_tokens=1000)\n",
    "    \n",
    "    return embedding_model, chat\n",
    "\n",
    "# Stored in langchain_pg_collection and langchain_pg_embedding as this\n",
    "def initialize_db(type, embedding_model):\n",
    "    '''\n",
    "    Initialize the database for storing the embeddings\n",
    "\n",
    "    Args:\n",
    "    type: str: Type of DB to use. Options are \"PGVECTOR\" or \"FAISS\"\n",
    "    embedding_model: AzureOpenAIEmbeddings: The embedding model to use\n",
    "\n",
    "    Returns:\n",
    "    db: PGVector or FAISS: The initialized database\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    db = {}\n",
    "\n",
    "    COLLECTION_NAME = f\"embedding\"\n",
    "\n",
    "    # Use Postgres DB if it has PGVector add-on\n",
    "    if type == \"PGVECTOR\":\n",
    "        CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "            driver=os.environ.get(\"POSTGRES_DRIVER\", \"psycopg2\"),\n",
    "            host=os.environ.get(\"POSTGRES_HOST\", \"localhost\"),\n",
    "            port=int(os.environ.get(\"POSTGRES_PORT\", \"5432\")),\n",
    "            database=os.environ.get(\"POSTGRES_DB\", \"postgres\"),\n",
    "            user=os.environ.get(\"POSTGRES_USER\", \"postgres\"),\n",
    "            password=os.environ.get(\"POSTGRES_PASSWORD\", \"postgres\"),\n",
    "        )\n",
    "        db = PGVector(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            connection_string=CONNECTION_STRING,\n",
    "            embedding_function=embedding_model,\n",
    "        )\n",
    "    # Basic FAISS\n",
    "    elif(type == \"FAISS\"):\n",
    "        dimensions: int = len(embedding_model.embed_query(\"dummy\"))\n",
    "        db = FAISS(\n",
    "            embedding_function=embedding_model,\n",
    "            index=IndexFlatL2(dimensions),\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={},\n",
    "            normalize_L2=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"Invalid DB type\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_document(content, metadata, db):\n",
    "    '''\n",
    "    \n",
    "    Add a document to the vecotr database along with its metadata\n",
    "\n",
    "    Args:\n",
    "\n",
    "    content: str: The content of the document\n",
    "    metadata: dict: The metadata of the document\n",
    "    db: PGVector or FAISS: The database to add the document to\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    metadata['custom_id'] = uuid_str\n",
    "\n",
    "    new_doc =  Document(\n",
    "        page_content=content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    id = db.add_documents(\n",
    "        [new_doc],\n",
    "        ids=[uuid_str]\n",
    "    )\n",
    "\n",
    "def read_data(filename):\n",
    "    '''\n",
    "    \n",
    "    Read data from an Excel file\n",
    "\n",
    "    Args:\n",
    "\n",
    "    filename: str: The name of the file to read\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    data: pd.DataFrame: The data read from the file\n",
    "    \n",
    "    '''\n",
    "    data = pd.read_excel(filename)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    data.columns = data.columns.str.lower()\n",
    "    data['id'] = data.index\n",
    "    data['id'] = data['id'].apply(lambda x: f'{filename}-{x}')\n",
    "    print(f\"{filename} > {data.shape}\")\n",
    "    return data\n",
    "\n",
    "def call_llm(instructions, prompt, chat, retries=5, retry_count=0):\n",
    "    '''\n",
    "    \n",
    "    Call the LLM model\n",
    "\n",
    "    Args:\n",
    "\n",
    "    instructions: str: Instructions to provide to the model\n",
    "    prompt: str: The prompt to provide to the model\n",
    "    chat: AzureChatOpenAI: The chat model to use\n",
    "    retries: int: Number of retries to attempt\n",
    "    retry_count: int: Current retry count\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    response: dict: The response from the model\n",
    "\n",
    "    '''\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=instructions\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=prompt\n",
    "        ),\n",
    "    ]\n",
    "    try:\n",
    "        response = chat(messages)\n",
    "        response= response.content\n",
    "        response = response.replace(\"```json\", \"\").replace(\"```\", \"\").replace(\"\\n\", \"\")\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except:\n",
    "            print(\"JSON didn't parse\")\n",
    "            print(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Error calling LLM\")\n",
    "        print(chat(messages))\n",
    "        retry_count += 1\n",
    "        if retry_count < retries:\n",
    "            sleep(1)\n",
    "            response = call_llm(instructions, prompt, chat, retries=retries, retry_count=retry_count)\n",
    "            print(response)\n",
    "            return response\n",
    "        else:\n",
    "            print(\"Error calling LLM. Max retries reached\")\n",
    "            return None\n",
    "\n",
    "def matching_fields(data1, data2, chat, context_prompt, format_prompt=\"\", force=False):\n",
    "    '''\n",
    "    \n",
    "    Get the fields to use for matching. This generates a JSON record which drives how matching\n",
    "    is done. It's controlled by prompts to determine fields types and output formats. Here are some examples ...\n",
    "\n",
    "    fields_context_prompt = \"\"\"\n",
    "        You looking at two dataframes data1 and data2 to see which columns can be used for matching mentors to mentees\n",
    "        Id fields like 'name', 'id' and 'email' CANNOT BE USED FOR MATCHING, exclude them in your response\n",
    "        URL fields like 'url' cannot be used for matching\n",
    "        Fields providing resume/CV locations cannot be used for matching\n",
    "        Fields related to skills/interests and demographics can be used for matching\n",
    "        Exclude fields related to strengths and weaknesses\n",
    "        Exclude mentor fields related to goals\n",
    "        IMPORTANT!!!! Exclude ward fields in location fields\n",
    "        Always include area of interest fields\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt used to decide output format when matching fields\n",
    "    fields_format_prompt = \"\"\"\n",
    "    Please reply with a JSON record in this format:\n",
    "\n",
    "        {\n",
    "            \"data1_fields\": {\n",
    "                \"skills_fields\": [<FIELD NAME>, ...],\n",
    "                \"location_fields\": [<FIELD NAME>, ...],\n",
    "                \"demographics_fields\": [<FIELD NAME>, ...],\n",
    "                \"preferred_mentorship_mode\": [<FIELD NAME>, ...]\n",
    "            },\n",
    "            \"data2_fields\": {\n",
    "                \"skills_fields\": [<FIELD NAME>, ...],\n",
    "                \"location_fields\": [<FIELD NAME>, ...],\n",
    "                \"demographics_fields\": [<FIELD NAME>, ...],\n",
    "                \"preferred_mentorship_mode\": [<FIELD NAME>, ...]\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    Note: It creates a JSON file saved into ./data. If this file exists it will use it, otherwise it will regenerate. \n",
    "    The force parameter will force a regeneration.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    data1: pd.DataFrame: The first dataset\n",
    "    data2: pd.DataFrame: The second dataset\n",
    "    chat: AzureChatOpenAI: The chat model to use\n",
    "    context_prompt: str: The context prompt to use, controls how fields are classified and matched\n",
    "    format_prompt: str: The format prompt for the output\n",
    "    force: bool: Whether to force re-creation of the matching fields\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    resp: dict: The response from the model\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if os.path.exists(MATCHING_FIELDS_JSON) and not force:\n",
    "        print(\"Found existing fields matching file! Using it ...\")\n",
    "        with open(MATCHING_FIELDS_JSON) as f:\n",
    "            resp = json.load(f)\n",
    "        print(json.dumps(resp, indent=4))    \n",
    "        return resp\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    data1_cols = list(data1.columns)\n",
    "    data2_cols = list(data2.columns)\n",
    "\n",
    "    prompt = context_prompt\n",
    "    prompt += f\"Given these two records, what fields would be good for matching?\"\n",
    "    prompt += f\"\\n\\nDATA1: {data1_cols}\"\n",
    "    prompt += f\"\\n\\nDATA2: {data2_cols}\"\n",
    "    prompt += f\"\\n\\nPlease reply with a JSON record in this format:\"\n",
    "    prompt += format_prompt\n",
    "    \n",
    "    resp = call_llm(\"\", prompt, chat)\n",
    "    print(json.dumps(resp, indent=4))    \n",
    "\n",
    "    with open(MATCHING_FIELDS_JSON, 'w') as f:\n",
    "        json.dump(resp, f, indent=4)\n",
    "\n",
    "    return resp\n",
    "\n",
    "def subset_data(data, fields):\n",
    "    '''\n",
    "    \n",
    "    Function to subset dataframe to a column list. Also drops empty columns.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    data - Pandas dataframe\n",
    "    fields - Columns to subset\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    data - Pandas dataframe of subset data\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    fields = list(set(fields) & set(data.columns))\n",
    "    data = data[fields]\n",
    "    # Drop any columns which are all null\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    return data\n",
    "\n",
    "def index_data(data_content, data_all, db):\n",
    "    '''\n",
    "\n",
    "    Adds documents to the vector databse, with their embeddings. \n",
    "\n",
    "    Args:\n",
    "\n",
    "    data_content: Pandas dataframe, data to be indexed, usually a subset of columns\n",
    "    data_all: Pandas data frame of the data but with all columns, used to store metadata for each embedding\n",
    "    db: Vecotr DB, see initialize_db above\n",
    "    '''\n",
    "    for index, row in data_all.iterrows():\n",
    "        rec = data_content.iloc[index].to_dict()\n",
    "        content = \"\"\n",
    "        # Remove keys\n",
    "        for r in rec:\n",
    "            content += f\"{rec[r]};\"\n",
    "        content = content.replace('\\n', ';')\n",
    "        metadata = row.to_dict()\n",
    "        add_document(content, metadata, db)\n",
    "\n",
    "def get_distance(addr1, addr2):\n",
    "    key = f\"{addr1}__{addr2}\"\n",
    "    with open(DIST_MATRIX) as f:\n",
    "        dist_matrix = json.load(f)\n",
    "    if key in dist_matrix:\n",
    "        return dist_matrix[key]\n",
    "    else:\n",
    "        return -9999999\n",
    "\n",
    "def calc_dist_matrix(addresses1, addresses2, skip_strings):\n",
    "    '''\n",
    "    For two lists of unique addresses or location strings, calculates their distance to build\n",
    "    a dictionary of address combinations and their distances, stored to file.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    addresses1 - List of from addresses/location fields\n",
    "    addresses2 - List of to addresses/location fields\n",
    "    skip_strings - List of strings which force records to be skipped\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    dist_matrix: Dictionary with key <addresses1>__<addresses2> and vlues of distance in km\n",
    "\n",
    "    Dictionary also stored in ./data.\n",
    "    '''\n",
    "    dist_matrix = {}\n",
    "    for addr1 in list(set(addresses1)):\n",
    "        for addr2 in list(set(addresses2)):\n",
    "            skip = False\n",
    "            if addr1 == '' or addr2 == '':\n",
    "                skip = True\n",
    "            for s in skip_strings:\n",
    "                if s in addr1 or s in addr2:\n",
    "                    skip = True\n",
    "                    break\n",
    "            if skip:\n",
    "                continue\n",
    "            key = f\"{addr1}__{addr2}\"\n",
    "            print(key)\n",
    "            if key not in dist_matrix:\n",
    "                if addr1 == addr2 or 'Mombasa Metropolitan' in addr2:\n",
    "                    dist_matrix[key]  = 0.0\n",
    "                else:\n",
    "                    dist = gmaps.distance_matrix(addr1,addr2, units='metric')['rows'][0]['elements'][0]\n",
    "                    if dist['status'] == 'OK':\n",
    "                        dist_matrix[key] = dist['distance']['value']/1000.\n",
    "                        print(f\"        Distance between {addr1} and {addr2} is {dist_matrix[key]} km\")\n",
    "                    else:\n",
    "                        dist_matrix[key] = None\n",
    "\n",
    "    with open('dist_matrix.json', 'w') as f:\n",
    "        json.dump(dist_matrix, f, indent=4) \n",
    "\n",
    "    return dist_matrix\n",
    "\n",
    "def set_addresses(data, fields):\n",
    "    '''\n",
    "    \n",
    "    Sets an 'address' field on a dataframe, by concatenating vlaues in address 'fields'\n",
    "\n",
    "    Args:\n",
    "\n",
    "    data - Pandas dataframe\n",
    "    fields - List of address-related fields\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    data - Parndas dataframe with extra column 'address'\n",
    "    \n",
    "    '''\n",
    "    data_add = data[fields]\n",
    "    addresses = data_add.apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)\n",
    "    addresses = addresses.str.lower()\n",
    "    data['address'] = addresses\n",
    "    return data\n",
    "\n",
    "def get_ai_check_fields(match_fields):\n",
    "    '''\n",
    "    \n",
    "    Using the fields match JSON record, gets a list of all fields that might be useful for the AI\n",
    "    match checker.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    match_fields - dictionary of mathing fields, as output by function `match_fields`\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    ai_check_fields - list of field names\n",
    "    '''\n",
    "\n",
    "    ai_check_fields = []\n",
    "    for k in match_fields:\n",
    "        ai_check_fields += match_fields[k]\n",
    "    print(ai_check_fields)\n",
    "    return ai_check_fields\n",
    "\n",
    "def check_pii(data, chat):\n",
    "    '''\n",
    "    \n",
    "    Check if data contains PII columns. Note, this is not perfect and should be used as a guide only.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    data: pd.DataFrame: The data to check for PII\n",
    "    prompt: str: The prompt to use to check for PII\n",
    "    chat: AzureChatOpenAI: The chat model to use\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    matching_fields_list: dict: The matching fields list\n",
    "    \n",
    "    '''\n",
    "\n",
    "    pii_prompt = \"\"\"\n",
    "\n",
    "    =================\n",
    "\n",
    "    Analyze the text above \n",
    "    Please identify whether not the above text contains any instances where a person's name is given \n",
    "    Please identify all email addresses \n",
    "    I need to find any exact dates that correspond to the day a person was born. They typically follow the format MM/DD/YYYY or DD/YY or other combinations.  They Must have a day number. Can you find them in the text? \n",
    "    Find any locations that could be a person's home address in this text. They must have a stret number or apartment number.\n",
    "\n",
    "    Your response should be a JSON record in this format:\n",
    "\n",
    "    {\n",
    "        \"has_pii\": <\"yes\" or \"no\">,\n",
    "        \"fields\": \"<FIELD>, ...\"\n",
    "        \"values\": \"<VALUE>, ...\"\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.sample(5)\n",
    "    data = data.to_dict(orient='records')\n",
    "    data = json.dumps(data, indent=4)\n",
    "\n",
    "    # Noting here we need a more powerful model to be safe\n",
    "    chat = AzureChatOpenAI(azure_deployment=\"gpt-4-turbo\", \n",
    "                    azure_endpoint=os.getenv(\"BASE_URL\"), \n",
    "                    temperature=0, max_tokens=1000)\n",
    "\n",
    "    # Check data1 for PII\n",
    "    print(\"\\n\\nChecking data for PII ...\")\n",
    "    prompt = data  + pii_prompt \n",
    "    print(prompt)\n",
    "\n",
    "    resp = call_llm(\"\", prompt, chat)\n",
    "\n",
    "    if resp['has_pii'] == 'yes':\n",
    "        print(\"PII detected!\")\n",
    "        print(f\"Fields: {resp['fields']}\")\n",
    "        print(f\"Values: {resp['values']}\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print(\"No PII detected\")\n",
    "\n",
    "def ai_check(data1, data2, chat, context_prompt=\"\"):\n",
    "    '''\n",
    "    \n",
    "    Checks a match as found by semantic search to see it it's still a match when considering more fields\n",
    "\n",
    "    Args:\n",
    "\n",
    "    data1 - JSON record of record1, being a subset of input table row by ai_match_fields\n",
    "    data2 - JSON record of record2, being a subset of input table row by ai_match_fields\n",
    "    context_prompt String, prompt to guide the AI matcher further\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    rep - dict, stating answer and reason\n",
    "    '''\n",
    "\n",
    "    prompt = context_prompt\n",
    "    prompt += f\"Given these two records, what fields would be good for matching?\"\n",
    "    prompt += f\"\\n\\nDATA1: {data1}\"\n",
    "    prompt += f\"\\n\\nDATA2: {data2}\"\n",
    "    prompt += f\"\\n\\nPlease reply with a JSON record in this format:\"\n",
    "    prompt += \"\"\"\n",
    "        {\n",
    "            \"answer\": <\"yes\" or \"no\">,\n",
    "            \"reason\": <\"reason for answer\">    \n",
    "        }\n",
    "    \"\"\"\n",
    "    resp = call_llm(\"\", prompt, chat)\n",
    "    print(json.dumps(resp, indent=4))    \n",
    "    return resp\n",
    "\n",
    "def run_matching_batch(data1, data2, rec1_name, rec2_name, fields_context_prompt, fields_format_prompt, ai_check_prompt, \\\n",
    "                       max_travel_distance_km=15, data2_cap=10, force=False):\n",
    "    \n",
    "    '''\n",
    "    Main batch function for calculating matching between two dataframes using a staged approach (i) Simple vector similarity\n",
    "    top k matches, then reviewed for distances between data, then finally assessed by AI match checker.\n",
    "    \n",
    "    Args:\n",
    "\n",
    "    data1 - Pandas dataframe of first dataset (eg mentors)\n",
    "    data2 - Pandas dataframe of second dataset to match against (eg mentees)\n",
    "    rec1_name - String name of first dataset, eg 'mentors'\n",
    "    rec2_name - String name of second dataset, eg 'menteess'\n",
    "    fields_context_prompt: str: The context prompt to use, controls how fields are classified and matched, see 'matching_fields' function\n",
    "    fields_format_prompt: str: The format prompt for the output, see 'matching_fields' function\n",
    "    ai_check_prompt: str, prompt to guide the AI matcher further, see ai_check function\n",
    "    max_traveL_distance_km: Maximum travel distance allowed for a match. If exceeded, match field <rec1_name>_distance_match is set to 'no'\n",
    "    data2_cap: Number of data2 records to try and match with all of data1\n",
    "    force: Bool, used to force refresh of matching_fields JSON record\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    matches: Pandas dataframe of each match with all data1 fields plus extra columns for the matched data2 fields\n",
    "\n",
    "    Note, the matches willl have a row per match, so mentees values are repeated for each mentor. The function also saves\n",
    "    matches to an excel file in ./data \n",
    "\n",
    "    '''\n",
    "\n",
    "    print(\"\\n=====> Setting up models ...\")\n",
    "    embedding_model, chat = setup_models()\n",
    "    db = initialize_db('FAISS', embedding_model)\n",
    "\n",
    "    print(\"\\n=====> Identifying match fields ...\")\n",
    "    matching_fields_list = matching_fields(data1, data2, chat, fields_context_prompt, fields_format_prompt, force=force)\n",
    "\n",
    "    # Please do not comment this out. :) \n",
    "    print(\"\\n=====> Checking for PII  ...\")\n",
    "    check_pii(data1,chat)\n",
    "    check_pii(data2,chat)\n",
    "\n",
    "    print(\"\\n=====> Calculating travel distance matrix ...\")\n",
    "    data1 = set_addresses(data1, matching_fields_list['data1_fields']['location_fields'])\n",
    "    data2 = set_addresses(data2, matching_fields_list['data2_fields']['location_fields'])\n",
    "    dist_matrix = calc_dist_matrix(data1['address'].unique(), data2['address'].unique(), skip_strings=['outside mombasa'])\n",
    "\n",
    "    print(\"\\n=====> Calculating index for semantic match ...\")\n",
    "    data1_search = subset_data(data1, matching_fields_list['data1_fields']['skills_fields'])\n",
    "    data2_search = subset_data(data2, matching_fields_list['data2_fields']['skills_fields'])\n",
    "    print(list(data1_search.columns))\n",
    "    data1_index = index_data(data1_search, data1, db)\n",
    "\n",
    "    print(\"\\n=====> Extracting AI check fields for datasets ...\")\n",
    "    ai_check_fields1 = get_ai_check_fields(matching_fields_list['data1_fields'])\n",
    "    ai_check_fields2 = get_ai_check_fields(matching_fields_list['data2_fields'])\n",
    "\n",
    "    print(\"\\n=====> Starting batch matching of mentees ...\")\n",
    "    matches = []\n",
    "    for index, row in data2[0:data2_cap].iterrows():\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Simple semantic search\n",
    "            data2_search_fields = list(data2_search.iloc[index]) \n",
    "            data2_search_string = \" \".join(data2_search_fields)\n",
    "            print(f\"\\n\\n{rec2_name}: {data2_search_string}\")\n",
    "            print(f\"{rec2_name} ID: {row['id']}\")\n",
    "            print(f\"{rec2_name} Address: {row['address']}\")\n",
    "            docs_and_scores = db.similarity_search_with_score(data2_search_string, top_k=3)\n",
    "            print(f\"RESULTS: {len(docs_and_scores)}\")\n",
    "\n",
    "            data2_address = row['address']\n",
    "            data2_ai_record = data2.iloc[index][ai_check_fields2].to_dict()\n",
    "            data2_ai_record = json.dumps(data2_ai_record,indent=4)\n",
    "            print(f\"{rec2_name} AI Fields:\\n\")\n",
    "            print(data2_ai_record)\n",
    "\n",
    "            # Filter simple results by secondary filters (distance and ai check)\n",
    "            data2['matches'] = \"\"\n",
    "            for d in docs_and_scores:\n",
    "\n",
    "                data1_skills = d[0].page_content\n",
    "                data1_full = d[0].metadata\n",
    "\n",
    "                # Flag if distances are too far\n",
    "                data1_address = data1_full['address']\n",
    "                travel_distance_km = get_distance(data1_address, data2_address)\n",
    "                distance_match = 'yes'\n",
    "                if travel_distance_km > max_travel_distance_km:\n",
    "                    print(\"Match found but distance too far\")\n",
    "                    distance_match = 'no'\n",
    "\n",
    "                print(f\"Score: {d[1]}\")\n",
    "                print(f\"{rec1_name} ID: {data1_full['id']}\")\n",
    "                print(f\"{rec1_name} match terms: {data1_skills}\")\n",
    "                print(f\"{rec1_name} location: {data1_full['address']}\")\n",
    "                print(f\"Distance to {rec1_name}: {travel_distance_km} km\")\n",
    "\n",
    "                data1_ai_record = pd.DataFrame([data1_full])\n",
    "                data1_ai_record = data1_ai_record[ai_check_fields1]\n",
    "                data1_ai_record = json.loads(data1_ai_record.to_json(orient=\"records\"))[0]\n",
    "                data1_ai_record = json.dumps(data1_ai_record,indent=4)\n",
    "                print(\"AI Check fields:\")\n",
    "                print(f\"   {rec2_name} : {data2_ai_record}\")\n",
    "                print(f\"   {rec1_name}: {data1_ai_record}\")\n",
    "                print(\"******* AI CHECK:\")\n",
    "                ai_check_result = ai_check(data1_ai_record, data2_ai_record, chat, ai_check_prompt)\n",
    "\n",
    "                # Build match record\n",
    "                r = {}\n",
    "                r['id'] = row['id']\n",
    "                r['address'] = data2_address\n",
    "                r[f'{rec1_name}_id'] = data1_full['id']\n",
    "                for f in json.loads(data1_ai_record):\n",
    "                    r[f'{rec1_name}_{f}'] = data1_full[f]\n",
    "                r[f'{rec1_name}_address'] = data1_address\n",
    "                r[f'{rec1_name}_score'] = d[1]\n",
    "                r[f'{rec1_name}_match_terms'] = data1_skills\n",
    "                r[f'{rec1_name}_distance'] = travel_distance_km\n",
    "                r[f'{rec1_name}_json_comparison'] = f\"MENTEE:\\n{data2_ai_record}\\n\\nMENTOR:\\n{data1_ai_record}\"\n",
    "                r[f'{rec1_name}_distance_match'] = distance_match\n",
    "                r[f'{rec1_name}_ai_check_result'] = ai_check_result['answer']\n",
    "                r[f'{rec1_name}_ai_check_reason'] = ai_check_result['reason']\n",
    "                matches.append(r)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {index}\")\n",
    "            print(traceback.format_exc())\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    if len(matches) > 0:\n",
    "        matches = pd.DataFrame(matches)\n",
    "        data2 = data2.merge(matches, on='id', how='left')\n",
    "        print(f\"Saving matches to {MATCHES_FILE}\")\n",
    "        data2.to_excel(MATCHES_FILE, index=False)\n",
    "\n",
    "    return data2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching batch for mentors and mentees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading in two dataframes to match ...\")\n",
    "mentors = read_data(MENTOR_DATA)\n",
    "# print(mentors.columns)\n",
    "mentees = read_data(MENTEE_DATA)\n",
    "# print(mentees.columns)\n",
    "\n",
    "# Prompt used when automatically identifying matching fields\n",
    "fields_context_prompt = \"\"\"\n",
    "    You looking at two dataframes data1 and data2 to see which columns can be used for matching mentors to mentees\n",
    "    Id fields like 'name', 'id', 'address' and 'email' CANNOT BE USED FOR MATCHING, exclude them in your response\n",
    "    URL fields like 'url' cannot be used for matching\n",
    "    Fields providing resume/CV locations cannot be used for matching\n",
    "    Fields related to skills/interests and demographics can be used for matching\n",
    "    Exclude fields related to strengths and weaknesses\n",
    "    Exclude mentor fields related to goals\n",
    "    IMPORTANT!!!! Exclude ward fields in location fields\n",
    "    Always include area of interest fields\n",
    "\"\"\"\n",
    "\n",
    "# Prompt used to decide output format when matching fields\n",
    "fields_format_prompt = \"\"\"\n",
    "Please reply with a JSON record in this format:\n",
    "\n",
    "        {\n",
    "            \"data1_fields\": {\n",
    "                \"skills_fields\": [<FIELD NAME>, ...],\n",
    "                \"location_fields\": [<FIELD NAME>, ...],\n",
    "                \"demographics_fields\": [<FIELD NAME>, ...],\n",
    "                \"preferred_mentorship_mode\": [<FIELD NAME>, ...]\n",
    "            },\n",
    "            \"data2_fields\": {\n",
    "                \"skills_fields\": [<FIELD NAME>, ...],\n",
    "                \"location_fields\": [<FIELD NAME>, ...],\n",
    "                \"demographics_fields\": [<FIELD NAME>, ...],\n",
    "                \"preferred_mentorship_mode\": [<FIELD NAME>, ...]\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "ai_check_prompt = \"\"\"\n",
    "    You are an AI checking potential matches between mentors and mentees\n",
    "    You ignore locations and focus on other fields\n",
    "    If mentor or mentee wants similar demographics, such as gender, age etc, be very careful comparing those fields\n",
    "    Given the above, is this mentor matched to this mentee?\n",
    "\"\"\"\n",
    "\n",
    "max_travel_distance_km = 15\n",
    "mentee_cap = 10\n",
    "\n",
    "matches = run_matching_batch(mentors, mentees, 'mentor', 'mentee', fields_context_prompt, \\\n",
    "                             fields_format_prompt, ai_check_prompt, max_travel_distance_km, mentee_cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goyn-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
